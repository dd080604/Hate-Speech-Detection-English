{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfa54efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "#%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a50b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nolenhuang/Hate Speech/Hate-Speech-Detection-English\n"
     ]
    }
   ],
   "source": [
    "#%cd /Users/nolenhuang/Hate Speech/Hate-Speech-Detection-English\n",
    "from hate_preproc import PreprocessConfig, preprocess_dataframe, add_signal_columns\n",
    "\n",
    "df = pd.read_csv(\"hate_speech_train.csv\") # might need to update to train.csv\n",
    "cfg = PreprocessConfig()\n",
    "df_clean = preprocess_dataframe(df, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245433f",
   "metadata": {},
   "source": [
    "Stratified 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf6e76fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: train=12000, val=3000, pos_rate_train=0.3000, pos_rate_val=0.3000\n",
      "Fold 2: train=12000, val=3000, pos_rate_train=0.3000, pos_rate_val=0.3000\n",
      "Fold 3: train=12000, val=3000, pos_rate_train=0.3000, pos_rate_val=0.3000\n",
      "Fold 4: train=12000, val=3000, pos_rate_train=0.3000, pos_rate_val=0.3000\n",
      "Fold 5: train=12000, val=3000, pos_rate_train=0.3000, pos_rate_val=0.3000\n"
     ]
    }
   ],
   "source": [
    "#use label(0/1) as y\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "# X as dummy to fit the length\n",
    "X_dummy = np.zeros(len(y))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_dummy, y), start=1):\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    print(\n",
    "        f\"Fold {fold}: \"\n",
    "        f\"train={len(train_idx)}, val={len(val_idx)}, \"\n",
    "        f\"pos_rate_train={y_train.mean():.4f}, pos_rate_val={y_val.mean():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3552cb",
   "metadata": {},
   "source": [
    "Count keyword for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f8cb1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_key_list_hate(df_in, size_table=200, ignore=3,\n",
    "                           text_col=\"text_clean\", label_col=\"label\",\n",
    "                           pos_label=1):\n",
    "    # dicts: total token counts per class, and document frequency\n",
    "    dict_pos = {}\n",
    "    dict_neg = {}\n",
    "    dict_df  = {}  # document frequency (how many docs contain the word)\n",
    "\n",
    "    n_docs = df_in.shape[0]\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        text = \"\" if pd.isna(df_in.iloc[i][text_col]) else str(df_in.iloc[i][text_col])\n",
    "        finds = re.findall(r\"[A-Za-z]+\", text)\n",
    "\n",
    "        is_pos = (df_in.iloc[i][label_col] == pos_label)\n",
    "\n",
    "        # token counts (TF numerator)\n",
    "        for w in finds:\n",
    "            if len(w) < ignore:\n",
    "                continue\n",
    "            w = w.lower()\n",
    "            if is_pos:\n",
    "                dict_pos[w] = dict_pos.get(w, 0) + 1\n",
    "                dict_neg[w] = dict_neg.get(w, 0)\n",
    "            else:\n",
    "                dict_neg[w] = dict_neg.get(w, 0) + 1\n",
    "                dict_pos[w] = dict_pos.get(w, 0)\n",
    "\n",
    "        # document frequency (IDF denominator)\n",
    "        word_set = set()\n",
    "        for w in finds:\n",
    "            if len(w) < ignore:\n",
    "                continue\n",
    "            w = w.lower()\n",
    "            if w not in word_set:\n",
    "                dict_df[w] = dict_df.get(w, 0) + 1\n",
    "                word_set.add(w)\n",
    "\n",
    "    # Build table\n",
    "    word_df = pd.DataFrame({\n",
    "        \"keyword\": list(dict_df.keys()),\n",
    "        \"neg_cnt\": [dict_neg.get(k, 0) for k in dict_df.keys()],\n",
    "        \"pos_cnt\": [dict_pos.get(k, 0) for k in dict_df.keys()],\n",
    "        \"df\":      [dict_df.get(k, 0)  for k in dict_df.keys()],\n",
    "    })\n",
    "\n",
    "    # class sizes\n",
    "    n_pos = (df_in[label_col] == pos_label).sum()\n",
    "    n_neg = n_docs - n_pos\n",
    "\n",
    "    # normalized TF (per document count in that class)\n",
    "    word_df[\"neg_tf\"] = word_df[\"neg_cnt\"].astype(float) / max(n_neg, 1)\n",
    "    word_df[\"pos_tf\"] = word_df[\"pos_cnt\"].astype(float) / max(n_pos, 1)\n",
    "\n",
    "    # IDF (same style as your example)\n",
    "    word_df[\"idf\"] = np.log10(word_df.shape[0] / word_df[\"df\"].astype(float).clip(lower=1.0))\n",
    "\n",
    "    # TF-IDF\n",
    "    word_df[\"neg_tfidf\"] = word_df[\"neg_tf\"] * word_df[\"idf\"]\n",
    "    word_df[\"pos_tfidf\"] = word_df[\"pos_tf\"] * word_df[\"idf\"]\n",
    "\n",
    "    # diff: \"more hate\" words rank higher\n",
    "    word_df[\"diff\"] = word_df[\"pos_tfidf\"] - word_df[\"neg_tfidf\"]\n",
    "\n",
    "    selected = word_df.sort_values(\"diff\", ascending=False).head(size_table)\n",
    "\n",
    "    keyword_dict = {w.strip(): idx for idx, w in enumerate(selected[\"keyword\"].tolist())}\n",
    "    return keyword_dict, selected, word_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead80cc",
   "metadata": {},
   "source": [
    "view keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76e56aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>neg_cnt</th>\n",
       "      <th>pos_cnt</th>\n",
       "      <th>df</th>\n",
       "      <th>neg_tf</th>\n",
       "      <th>pos_tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>neg_tfidf</th>\n",
       "      <th>pos_tfidf</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>bitch</td>\n",
       "      <td>567</td>\n",
       "      <td>1631</td>\n",
       "      <td>2083</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.362444</td>\n",
       "      <td>0.947989</td>\n",
       "      <td>0.051191</td>\n",
       "      <td>0.343593</td>\n",
       "      <td>0.292402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>bitches</td>\n",
       "      <td>248</td>\n",
       "      <td>617</td>\n",
       "      <td>824</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>0.137111</td>\n",
       "      <td>1.350751</td>\n",
       "      <td>0.031903</td>\n",
       "      <td>0.185203</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>hoes</td>\n",
       "      <td>192</td>\n",
       "      <td>487</td>\n",
       "      <td>648</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.108222</td>\n",
       "      <td>1.455103</td>\n",
       "      <td>0.026608</td>\n",
       "      <td>0.157475</td>\n",
       "      <td>0.130867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>pussy</td>\n",
       "      <td>135</td>\n",
       "      <td>413</td>\n",
       "      <td>524</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>0.091778</td>\n",
       "      <td>1.547347</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.142012</td>\n",
       "      <td>0.122118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hoe</td>\n",
       "      <td>159</td>\n",
       "      <td>390</td>\n",
       "      <td>516</td>\n",
       "      <td>0.015143</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>1.554029</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.134682</td>\n",
       "      <td>0.111150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>nigga</td>\n",
       "      <td>56</td>\n",
       "      <td>288</td>\n",
       "      <td>317</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>1.765619</td>\n",
       "      <td>0.009417</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>ass</td>\n",
       "      <td>158</td>\n",
       "      <td>338</td>\n",
       "      <td>457</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>0.075111</td>\n",
       "      <td>1.606762</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>0.120686</td>\n",
       "      <td>0.096508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>fuck</td>\n",
       "      <td>129</td>\n",
       "      <td>308</td>\n",
       "      <td>409</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.068444</td>\n",
       "      <td>1.654955</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.113272</td>\n",
       "      <td>0.092940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>don</td>\n",
       "      <td>102</td>\n",
       "      <td>274</td>\n",
       "      <td>357</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.060889</td>\n",
       "      <td>1.714010</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.104364</td>\n",
       "      <td>0.087714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>shit</td>\n",
       "      <td>126</td>\n",
       "      <td>258</td>\n",
       "      <td>366</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.057333</td>\n",
       "      <td>1.703197</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.097650</td>\n",
       "      <td>0.077212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>faggot</td>\n",
       "      <td>12</td>\n",
       "      <td>178</td>\n",
       "      <td>189</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>1.990217</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.078724</td>\n",
       "      <td>0.076450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>ain</td>\n",
       "      <td>17</td>\n",
       "      <td>164</td>\n",
       "      <td>174</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.036444</td>\n",
       "      <td>2.026129</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.073841</td>\n",
       "      <td>0.070561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>niggas</td>\n",
       "      <td>50</td>\n",
       "      <td>186</td>\n",
       "      <td>222</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>1.920325</td>\n",
       "      <td>0.009144</td>\n",
       "      <td>0.079373</td>\n",
       "      <td>0.070229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>lol</td>\n",
       "      <td>106</td>\n",
       "      <td>191</td>\n",
       "      <td>286</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.042444</td>\n",
       "      <td>1.810312</td>\n",
       "      <td>0.018276</td>\n",
       "      <td>0.076838</td>\n",
       "      <td>0.058562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>fucking</td>\n",
       "      <td>77</td>\n",
       "      <td>150</td>\n",
       "      <td>222</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.920325</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>0.064011</td>\n",
       "      <td>0.049928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>was</td>\n",
       "      <td>171</td>\n",
       "      <td>199</td>\n",
       "      <td>336</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.044222</td>\n",
       "      <td>1.740339</td>\n",
       "      <td>0.028343</td>\n",
       "      <td>0.076962</td>\n",
       "      <td>0.048619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>nigger</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>2.253841</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.048583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>got</td>\n",
       "      <td>277</td>\n",
       "      <td>246</td>\n",
       "      <td>500</td>\n",
       "      <td>0.026381</td>\n",
       "      <td>0.054667</td>\n",
       "      <td>1.567708</td>\n",
       "      <td>0.041358</td>\n",
       "      <td>0.085701</td>\n",
       "      <td>0.044344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>these</td>\n",
       "      <td>314</td>\n",
       "      <td>262</td>\n",
       "      <td>554</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>0.058222</td>\n",
       "      <td>1.523169</td>\n",
       "      <td>0.045550</td>\n",
       "      <td>0.088682</td>\n",
       "      <td>0.043132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>dick</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.018444</td>\n",
       "      <td>2.327159</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.042923</td>\n",
       "      <td>0.041372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword  neg_cnt  pos_cnt    df    neg_tf    pos_tf       idf  \\\n",
       "208     bitch      567     1631  2083  0.054000  0.362444  0.947989   \n",
       "95    bitches      248      617   824  0.023619  0.137111  1.350751   \n",
       "78       hoes      192      487   648  0.018286  0.108222  1.455103   \n",
       "368     pussy      135      413   524  0.012857  0.091778  1.547347   \n",
       "31        hoe      159      390   516  0.015143  0.086667  1.554029   \n",
       "110     nigga       56      288   317  0.005333  0.064000  1.765619   \n",
       "650       ass      158      338   457  0.015048  0.075111  1.606762   \n",
       "648      fuck      129      308   409  0.012286  0.068444  1.654955   \n",
       "364       don      102      274   357  0.009714  0.060889  1.714010   \n",
       "232      shit      126      258   366  0.012000  0.057333  1.703197   \n",
       "43     faggot       12      178   189  0.001143  0.039556  1.990217   \n",
       "336       ain       17      164   174  0.001619  0.036444  2.026129   \n",
       "485    niggas       50      186   222  0.004762  0.041333  1.920325   \n",
       "284       lol      106      191   286  0.010095  0.042444  1.810312   \n",
       "570   fucking       77      150   222  0.007333  0.033333  1.920325   \n",
       "262       was      171      199   336  0.016286  0.044222  1.740339   \n",
       "1058   nigger        7      100   103  0.000667  0.022222  2.253841   \n",
       "112       got      277      246   500  0.026381  0.054667  1.567708   \n",
       "370     these      314      262   554  0.029905  0.058222  1.523169   \n",
       "528      dick        7       83    87  0.000667  0.018444  2.327159   \n",
       "\n",
       "      neg_tfidf  pos_tfidf      diff  \n",
       "208    0.051191   0.343593  0.292402  \n",
       "95     0.031903   0.185203  0.153300  \n",
       "78     0.026608   0.157475  0.130867  \n",
       "368    0.019894   0.142012  0.122118  \n",
       "31     0.023532   0.134682  0.111150  \n",
       "110    0.009417   0.113000  0.103583  \n",
       "650    0.024178   0.120686  0.096508  \n",
       "648    0.020332   0.113272  0.092940  \n",
       "364    0.016650   0.104364  0.087714  \n",
       "232    0.020438   0.097650  0.077212  \n",
       "43     0.002275   0.078724  0.076450  \n",
       "336    0.003280   0.073841  0.070561  \n",
       "485    0.009144   0.079373  0.070229  \n",
       "284    0.018276   0.076838  0.058562  \n",
       "570    0.014082   0.064011  0.049928  \n",
       "262    0.028343   0.076962  0.048619  \n",
       "1058   0.001503   0.050085  0.048583  \n",
       "112    0.041358   0.085701  0.044344  \n",
       "370    0.045550   0.088682  0.043132  \n",
       "528    0.001551   0.042923  0.041372  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_table = 300\n",
    "word_len_ignored = 3\n",
    "\n",
    "keyword_dict, top_words, word_table = generate_key_list_hate(\n",
    "    df_clean,\n",
    "    size_table=size_table,\n",
    "    ignore=word_len_ignored,\n",
    "    text_col=\"text_clean\",\n",
    "    label_col=\"label\",\n",
    "    pos_label=1\n",
    ")\n",
    "\n",
    "print(len(keyword_dict))\n",
    "top_words.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2c36d",
   "metadata": {},
   "source": [
    "'neg_cnt': # in 'label = 0' text\n",
    "'pos_cnt': # in 'label = 1' text\n",
    "'df': # appear in text (one count per text)\n",
    "'neg_tf': frequency rate in \"label = 0'\n",
    "'pos_tf': frequency rate in \"label = 1'\n",
    "'diff'ï¼š'pos_tfidf - neg_tfidf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e01bdf",
   "metadata": {},
   "source": [
    "Text to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37ff331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_vec(text, keyword_dict):\n",
    "    m = len(keyword_dict)\n",
    "    res = np.zeros(m, dtype=np.int32)\n",
    "\n",
    "    text = \"\" if text is None else str(text)\n",
    "    finds = re.findall(r\"[A-Za-z]+\", text)\n",
    "\n",
    "    for w in finds:\n",
    "        w = w.lower()\n",
    "        if w in keyword_dict:\n",
    "            res[keyword_dict[w]] = 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4153c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_features(df_in, keyword_dict, text_col=\"text_clean\", label_col=\"label\", has_label=True):\n",
    "    n = df_in.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "\n",
    "    X = np.zeros((n, m), dtype=np.int32)\n",
    "\n",
    "    for i in range(n):\n",
    "        X[i, :] = convert_text_to_vec(df_in.iloc[i][text_col], keyword_dict)\n",
    "\n",
    "    if has_label:\n",
    "        y = df_in[label_col].to_numpy().astype(int)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53d15538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train features\n",
    "X, y = df_to_features(df_clean, keyword_dict, text_col=\"text_clean\", has_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec1e1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_csv(\"hate_speech_test.csv\")\n",
    "#df_test_clean = preprocess_dataframe(df_test, cfg)\n",
    "\n",
    "#X_test = df_to_features(df_test_clean, keyword_dict, text_col=\"text_clean\", has_label=False)\n",
    "#print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01707",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77214f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval(model, X_train, y_train):\n",
    "    # fit\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict on training data (baseline check)\n",
    "    y_hat = model.predict(X_train)\n",
    "\n",
    "    # metrics \n",
    "    metrics = {\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, y_hat)),\n",
    "        \"train_confusion_matrix\": confusion_matrix(y_train, y_hat)\n",
    "    }\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "516590b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13acbbd",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c36b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model, metrics = fit_and_eval(model, X, y)\n",
    "results.append({\"model_name\": \"LogisticRegression\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc52fb7",
   "metadata": {},
   "source": [
    "##BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b00d9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "model, metrics = fit_and_eval(model, X, y)\n",
    "results.append({\"model_name\": \"BernoulliNB\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ee9cf",
   "metadata": {},
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c9fca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=0\n",
    ")\n",
    "model, metrics = fit_and_eval(model, X, y)\n",
    "\n",
    "results.append({\"model_name\": \"Random Forest\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f4baa",
   "metadata": {},
   "source": [
    "Linear SVM(faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cac2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(random_state=42)\n",
    "model, metrics = fit_and_eval(model, X, y)\n",
    "\n",
    "results.append({\"model_name\": \"Linear SVM\", **metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b120b46",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36c6bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "model, metrics = fit_and_eval(model, X, y)\n",
    "\n",
    "results.append({\"model_name\": \"SVM\", **metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32f8ab",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82d2dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model, metrics = fit_and_eval(model, X, y)\n",
    "results.append({\"model_name\": \"XGB\", **metrics})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4680b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.882467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.845667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.945067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.885733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.893000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name  train_accuracy\n",
       "0  LogisticRegression        0.882467\n",
       "1         BernoulliNB        0.845667\n",
       "2       Random Forest        0.945067\n",
       "3          Linear SVM        0.885733\n",
       "4                 SVM        0.925000\n",
       "5                 XGB        0.893000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)[[\"model_name\", \"train_accuracy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72e9f0",
   "metadata": {},
   "source": [
    "Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bdd070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# choose model\n",
    "#model = BernoulliNB()\n",
    "#model = LogisticRegression(max_iter=4000, class_weight=\"balanced\", random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "#model = LinearSVC(random_state=42)\n",
    "#model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "'''\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "'''\n",
    "# train features\n",
    "X_train, y_train = df_to_features(df_clean, keyword_dict, text_col=\"text_clean\", has_label=True)\n",
    "\n",
    "# test features\n",
    "df_test = pd.read_csv(\"hate_speech_test.csv\") # might need to update to test.csv\n",
    "df_test_clean = preprocess_dataframe(df_test, cfg)\n",
    "X_test = df_to_features(df_test_clean, keyword_dict, text_col=\"text_clean\", has_label=False)\n",
    "\n",
    "# fit + predict\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# submission (must be columns: id, label)\n",
    "predictions = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"].values,\n",
    "    \"label\": y_pred.astype(int)\n",
    "})\n",
    "\n",
    "#Kaggle rule: index = FALSE\n",
    "predictions.to_csv(\"submission.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7e989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
